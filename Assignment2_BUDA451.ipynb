{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Assignment 2 BUDA 451\"\n",
        "author: \"Collin Edwards\"\n",
        "date: \"today\"\n",
        "format: html\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Data processing, Model overfitting, Validation and Evaluation\n",
        "\n",
        "## Problem 1\n",
        "\n",
        "### Problem 1a\n",
        "\n",
        "#### What is the difference between dimensional reduction and feature subset selection?\n",
        "\n",
        "**Dimensionality reduction**\n",
        "\n",
        ":   This technique transforms the original high-dimensional feature space into a lower-dimensional space. It does this by creating new features that are combinations of the original features that captures most of the variance . For example, *Principal Component Analysis (PCA)* is a common method used for dimensionality reduction.\n",
        "\n",
        "**Feature subset selection**\n",
        "\n",
        ":   For this approach it selects a subset of the original features without transforming them. The goal is to identify and retain the most relevant and informative features while discarding the less important ones. This can be done using methods like *forward selection*, *backward elimination*, or *recursive feature elimination*.\n",
        "\n",
        "[The main difference is that dimensionality reduction creates new features, while feature subset selection retains the original features.]{.mark}\n",
        "\n",
        "### Problem 1b\n",
        "\n",
        "#### What are the possible reasons for model overfitting?\n",
        "\n",
        "1.  **Complex Models**: Using a model that is too complex for the amount of data available can lead to overfitting. For example, a deep neural network with many layers may learn to memorize the training data instead of generalizing from it.\n",
        "2.  **Insufficient Data**: When the training dataset is too small, the model may learn noise and outliers instead of the underlying patterns.\n",
        "3.  **Lack of Regularization**: Regularization techniques (like L1 or L2 regularization) help to penalize complex models. Without them, the model may fit the training data too closely.\n",
        "4.  **Noisy Data**: If the training data contains a lot of noise or outliers, the model may learn to fit these anomalies rather than the true underlying patterns.\n",
        "5.  **Inadequate Cross-Validation**: If the model is not properly validated using techniques like k-fold cross-validation, it may perform well on the training data but poorly on unseen data.\n",
        "\n",
        "### Problem 1c\n",
        "\n",
        "#### What can you do to avoid overfitting? Elaborate you answers with *Decision Trees*, and *Logistic Regression*.\n",
        "\n",
        "1.  **Decision Trees**: To avoid overfitting in decision trees, you can use techniques such as: Pruning[^1], setting a maximum depth[^2], requiring a minimum number of samples per leaf, or enforcing a minimum impurity decrease to restrict tree complexity.\n",
        "\n",
        "2.  **Logistic Regression**: To avoid overfitting in logistic regression, you can:\n",
        "\n",
        "    -   Use regularization techniques such as L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients.\n",
        "    -   Feature selection to remove irrelevant or redundant features.\n",
        "    -   Cross-validation to ensure that the model generalizes well to unseen data.\n",
        "\n",
        "[^1]: Pruning is a technique used in decision trees to remove branches that have little importance and do not provide significant power to the model. This helps in reducing the complexity of the tree and prevents it from fitting noise in the training data.\n",
        "\n",
        "[^2]: Setting a maximum depth in decision trees restricts the number of levels in the tree. This prevents the model from becoming too complex and helps it generalize better to unseen data.\n",
        "\n",
        "### Problem 1d\n",
        "\n",
        "#### What’s the difference between validation dataset and test dataset?\n",
        "\n",
        "**Validation Dataset**: This dataset is used during the training process to tune the model's hyperparameters and make decisions about the model architecture. It helps in selecting the best model among different candidates. The validation set is not used for training but is used to evaluate the model's performance during training.\n",
        "\n",
        "**Test Dataset**: This dataset is used to evaluate the final performance of the model after it has been trained and validated. It provides an unbiased estimate of the model's performance on unseen data. The test set should not be used in any way during the training or validation process.\n",
        "\n",
        "### Problem 1e\n",
        "\n",
        "#### Describe 10 fold-cross validation. What’s the advantage(s) and disadvantage(s) as compared to one train-test split in model evolution?\n",
        "\n",
        "**10-Fold Cross-Validation**: In this method, the dataset is divided into 10 equal parts (or folds). The model is trained on 9 folds and validated on the remaining fold. This process is repeated 10 times, with each fold being used as the validation set once. The final performance metric is usually the average of the performance across all 10 folds.\n",
        "\n",
        "**Advantages**: 1. **More Reliable Estimate**: It provides a more reliable estimate of the model's performance since it uses multiple train-test splits. 2. **Better Use of Data**: All data points are used for both training and validation, which is especially useful when the dataset is small. 3. **Reduces Variance**: It reduces the variance associated with a single train-test split, making the performance estimate more stable.\n",
        "\n",
        "**Disadvantages**: 1. **Computationally Expensive**: It requires more computational resources and time since the model is trained multiple times. 2. **Complexity**: It adds complexity to the model evaluation process, making it harder to implement and interpret.\n",
        "\n",
        "### Problem 1f\n",
        "\n",
        "#### Why can accuracy be a bad metric to evaluate your classifier? What other metrics you can use?\n",
        "\n",
        "**Issues with Accuracy**\n",
        "\n",
        ":   Accuracy can be misleading, especially in imbalanced datasets where one class significantly outnumbers the other. For example, if 95% of the data belongs to one class, a model that predicts the majority class will have 95% accuracy but will not be useful for predicting the minority class. It doesnt show how many false positives or false negatives there are. **Other Metrics**: 1. **Precision**: Measures the proportion of true positive predictions among all positive predictions. It is useful when the cost of false positives is high. 2. **Recall (Sensitivity)**: Measures the proportion of true positive predictions among all actual positive instances. It is useful when the cost of false negatives is high. 3. **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two metrics. 4. **ROC-AUC**: The area under the receiver operating characteristic curve, which provides a measure of the model's ability to distinguish between classes.\n",
        "\n",
        "## Problem 2 Decision Trees\n",
        "\n",
        "Consider the training examples shown bellow for a binary classification problem.\n",
        "\n",
        "| Movie ID | Format | Movie Category | Class |\n",
        "|:-------------|--------|----------------|-------|\n",
        "| 1            | DVD    | Entertainment  | C0    |\n",
        "| 2            | DVD    | Comedy         | C0    |\n",
        "| 3            | DVD    | Documentaries  | C0    |\n",
        "| 4            | DVD    | Comedy         | C0    |\n",
        "| 5            | DVD    | Comedy         | C0    |\n",
        "| 6            | DVD    | Comedy         | C0    |\n",
        "| 7            | Online | Comedy         | C0    |\n",
        "| 8            | Online | Comedy         | C0    |\n",
        "| 9            | Online | Comedy         | C0    |\n",
        "| 10           | Online | Documentaries  | C0    |\n",
        "| 11           | DVD    | Comedy         | C1    |\n",
        "| 12           | DVD    | Entertainment  | C1    |\n",
        "| 13           | Online | Entertainment  | C1    |\n",
        "| 14           | Online | Documentaries  | C1    |\n",
        "| 15           | Online | Documentaries  | C1    |\n",
        "| 16           | Online | Documentaries  | C1    |\n",
        "| 17           | Online | Documentaries  | C1    |\n",
        "| 18           | Online | Entertainment  | C1    |\n",
        "| 19           | Online | Documentaries  | C1    |\n",
        "| 20           | Online | Documentaries  | C1    |\n",
        "\n",
        "\n",
        "\n",
        "#### Problem 2a Compute the Entropy for the **Movie ID** attribute.\n",
        "\n",
        "Entropy is given by the formula:\n",
        "\n",
        "$$Entropy = - \\sum_{i=0}^{C-1} p_i(t) \\log_2 p_i(t)$$\n",
        "\n",
        "where $p_i(t)$ is the proportion of the $i$-th class in the node $t$ and $C$ is the number of classes.\n",
        "Below we define a helper function in Python and compute the overall entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def entropy(counts):\n",
        "    counts = np.array(counts)\n",
        "    total = counts.sum()\n",
        "    probabilities = counts / total\n",
        "    probabilities = probabilities[probabilities > 0]  # avoid log2(0)\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "# Overall dataset: 10 of C0 and 10 of C1\n",
        "overall_entropy = entropy([10, 10])\n",
        "print(\"Overall Entropy:\", overall_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we have two classes: $C0$ and $C1$. The entropy for the **Movie ID** attribute can be calculated as follows: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "movieid_entropy = 0\n",
        "print(\"Movie ID Entropy:\", movieid_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each movie ID is unique, so the entropy for the **Movie ID** attribute is 0.\n",
        "\n",
        "#### Problem 2b Compute the Entropy for the **Format** attribute.\n",
        "We calculate the entropy for each group and then compute the weighted average to get the overall entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Entropy for DVD group\n",
        "dvd_entropy = entropy([6, 2])\n",
        "# Entropy for Online group\n",
        "online_entropy = entropy([4, 8])\n",
        "\n",
        "# Weighted entropy for Format attribute\n",
        "format_entropy = (8/20) * dvd_entropy + (12/20) * online_entropy\n",
        "\n",
        "print(\"DVD Entropy:\", dvd_entropy)\n",
        "print(\"Online Entropy:\", online_entropy)\n",
        "print(\"Format Weighted Entropy:\", format_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Problem 2c Compute the Entropy for the **Movie Category** attribute using multiway split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Entropy for Entertainment\n",
        "entertainment_entropy = entropy([2, 2]) # 4 Examples of it\n",
        "# Entropy for Comedy\n",
        "comedy_entropy = entropy([6, 1]) # 7 Examples of it\n",
        "# Entropy for Documentaries\n",
        "documentaries_entropy = entropy([2, 7]) # 9 Examples of it\n",
        "\n",
        "# Weighted entropy for Movie Category attribute\n",
        "movie_category_entropy = (4/20) * entertainment_entropy + (7/20) * comedy_entropy + (9/20) * documentaries_entropy\n",
        "\n",
        "print(\"Entertainment Entropy:\", entertainment_entropy)\n",
        "print(\"Comedy Entropy:\", comedy_entropy)\n",
        "print(\"Documentaries Entropy:\", documentaries_entropy)\n",
        "print(\"Movie Category Weighted Entropy:\", movie_category_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Problem 2d Which of the three attributes has the lowest entropy?\n",
        "\n",
        "The attribute with the lowest entropy is the one that provides the most information gain when used for splitting the data. In this case, the attribute with the lowest entropy is the one that has the highest purity when the data is split based on its values. The attribute with the lowest entropy is the one that has the highest information gain when used for splitting the data.\n",
        "\n",
        "In this case, the **Movie Category** attribute has the lowest entropy since it has near 0.75 bits, which means it provides the most information gain when used for splitting the data. \n",
        "\n",
        "#### Problem 2e Which of the three attributes will you use for splitting the root node? Briefly explain your choice.\n",
        "\n",
        "The attribute that we should use for splitting the root node is the one that provides the most information gain. In this case, the **Movie Category** attribute has the lowest entropy, which means it provides the most information gain when used for splitting the data. By splitting the root node based on the **Movie Category** attribute, we can create child nodes that are more homogeneous and have higher purity.\n",
        "\n",
        "\n",
        "#### Problem 2f Decision tree evaulation. What's the accuracy of the decision tree model on the test data?\n",
        "\n",
        ":::\n",
        "![Decision Tree and Test Dataset](images/decision_tree.png){fig-align=\"center\" fig-cap=\"(a) A decision tree and (b) test dataset.\"}\n",
        ":::\n",
        "\n",
        "The decision tree model shown in the figure above is used to classify the test dataset. The test dataset consists of the following examples:### Decision Tree Structure\n",
        "\n",
        "1. **Root Node (A)**  \n",
        "   - If A = 0, go to node **B**  \n",
        "   - If A = 1, go to node **C**\n",
        "\n",
        "2. **Node B**  \n",
        "   - If B = 0, predict **+**  \n",
        "   - If B = 1, predict **–**\n",
        "\n",
        "3. **Node C**  \n",
        "   - If C = 0, predict **+**  \n",
        "   - If C = 1, predict **–**\n",
        "\n",
        "##### Classification of Each Instance\n",
        "\n",
        "1. **Instance 1**: (A=1, B=0, C=0, Class=+)  \n",
        "   - Path: A=1 → node C; C=0 → predict +  \n",
        "   - Actual class is + → **Correct**  \n",
        "\n",
        "2. **Instance 2**: (A=0, B=1, C=1, Class=+)  \n",
        "   - Path: A=0 → node B; B=1 → predict –  \n",
        "   - Actual class is + → **Incorrect**  \n",
        "\n",
        "3. **Instance 3**: (A=1, B=1, C=0, Class=+)  \n",
        "   - Path: A=1 → node C; C=0 → predict +  \n",
        "   - Actual class is + → **Correct**  \n",
        "\n",
        "4. **Instance 4**: (A=1, B=0, C=1, Class=–)  \n",
        "   - Path: A=1 → node C; C=1 → predict –  \n",
        "   - Actual class is – → **Correct**  \n",
        "\n",
        "5. **Instance 5**: (A=1, B=0, C=0, Class=+)  \n",
        "   - Path: A=1 → node C; C=0 → predict +  \n",
        "   - Actual class is + → **Correct**  \n",
        "\n",
        "### (f) Accuracy of the Decision Tree Model on the Test Data\n",
        "\n",
        "Out of 5 instances:\n",
        "\n",
        "- **4 are correctly classified** (Instances 1, 3, 4, and 5).\n",
        "- **1 is misclassified** (Instance 2).\n",
        "\n",
        "Thus, the accuracy is:\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Classifications}}{\\text{Total Number of Instances}} \n",
        "= \\frac{4}{5} = 0.80 = 80\\%$$\n",
        "\n",
        "Hence, the **accuracy of the decision tree on the test set is 80%**.\n",
        "\n",
        "## Problem 3 Linear models\n",
        "\n",
        "### Problem 3a What are the parameters for a generic logistic model as follows?\n",
        "\n",
        "$$z = b + w_1 x_1 + w_2 x_2 + \\dots + w_d x_d, \\quad p(y=1 \\mid x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "The parameters for a generic logistic model are:\n",
        "\n",
        "- **Bias Term ($b$)**: The bias term is the intercept of the model and represents the value of the output when all input features are zero.\n",
        "- **Weights ($w_1, w_2, \\dots, w_d$)**: The weights are the coefficients associated with each input feature $x_1, x_2, \\dots, x_d$. They determine the impact of each feature on the output.\n",
        "- **Logistic Function ($\\sigma(z)$)**: The logistic function is used to map the linear combination of input features and weights to a probability value between 0 and 1. It is defined as $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n",
        "\n",
        "### Problem 3b  Consider the following 15 points in a two-dimensional feature space (x1,x2) with labels as indicated (green + is positive, red ×is negative) in Figure 1. Let’s assume that after training the logistic regression model, you get the decision function as follows:\n",
        "\n",
        "$$z(x_1,x_2) = 4x_1+6x_2-24$$\n",
        "\n",
        ":::\n",
        "![two-dimensional feature space plot](images/Assignment2_Figure_1.png){fig-align=\"center\" fig-cap=\"(a) Plots of 15 points in a two-dimensional feature space.\"}\n",
        ":::\n",
        "\n",
        "#### Problem 3b-1 What is the zvalue of the decision function for point A? And what’s the probability of point as a positive example?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "\n",
        "# 1. Define the logistic regression decision function\n",
        "def decision_function(x1, x2, b=-24, w1=4, w2=6):\n",
        "    \"\"\"\n",
        "    Returns z = b + w1*x1 + w2*x2\n",
        "    \"\"\"\n",
        "    return b + w1*x1 + w2*x2\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Sigmoid function: 1 / (1 + e^(-z))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + math.exp(-z))\n",
        "\n",
        "# (b-1) Compute z and probability for point A\n",
        "# Let's assume point A has coordinates (x1=3, x2=3) as an example\n",
        "xA1, xA2 = 3, 3\n",
        "zA = decision_function(xA1, xA2)\n",
        "pA = sigmoid(zA)\n",
        "\n",
        "print(\"Point A =\", (xA1, xA2))\n",
        "print(\"z(A) =\", zA)\n",
        "print(\"Probability(A is positive) =\", pA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a decision function that essentially calculates the value of $z$ for a given set of input features $(x_1, x_2)$. We then use this function to compute the value of $z$ for point A, which has coordinates $(3, 3)$. Finally, we apply the sigmoid function to $z$ to obtain the probability of point A being classified as a positive example.\n",
        "\n",
        "#### Problem 3b-2 In the figure above, draw the decision boundary, z(x1,x2) = 0, and indicate the positive and negative regions (half-planes).\n",
        "\n",
        "The decision boundary is the line where the decision function $z(x_1, x_2) = 0$. This boundary separates the feature space into two regions: one where $z > 0$ (positive region) and one where $z < 0$ (negative region). The decision boundary is the set of points where the probability of being positive is equal to the probability of being negative.\n",
        "\n",
        "$$4x_1 + 6x_2 - 24 = 0 \\quad\\Longrightarrow\\quad 6x_2 = 24 - 4x_1 \\quad\\Longrightarrow\\quad x_2 = 4 - \\frac{2}{3} x_1.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the decision boundary: z(x1, x2) = 0\n",
        "x1 = np.linspace(0, 10, 200)\n",
        "x2 = 4 - (2/3)*x1\n",
        "\n",
        "# Example points with known labels\n",
        "data_points = [\n",
        "    (3, 3, 1),  # label=1 means positive\n",
        "    (5, 0.3, 0),  # label=0 means negative\n",
        "    (2, 4, 1),\n",
        "    (7, 1, 0)\n",
        "]\n",
        "\n",
        "## Re-create the boundary plot\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x1, x2, 'b-', label=\"Decision Boundary: z=0\")\n",
        "plt.fill_between(x1, x2, 10, color='lightblue', alpha=0.3, label='Predicted Positive (z>0)')\n",
        "plt.fill_between(x1, x2, 0, color='lightcoral', alpha=0.3, label='Predicted Negative (z<0)')\n",
        "\n",
        "# Plot data points\n",
        "for (px1, px2, label) in data_points:\n",
        "    if label == 1:\n",
        "        plt.plot(px1, px2, 'go', markersize=8, label=\"Positive\" if 'Pos' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
        "    else:\n",
        "        plt.plot(px1, px2, 'rx', markersize=8, label=\"Negative\" if 'Neg' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
        "\n",
        "plt.axhline(0, color='black', linewidth=0.5)\n",
        "plt.axvline(0, color='black', linewidth=0.5)\n",
        "plt.xlim(0, 10)\n",
        "plt.ylim(0, 10)\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.title('Logistic Regression Decision Boundary with Data Points')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows the decision boundary $z(x_1, x_2) = 0$ as a blue line. The half-planes are shaded to indicate the regions where the logistic regression model predicts positive (light blue) and negative (light coral) examples. The green circles represent positive examples, while the red crosses represent negative examples.\n",
        "\n",
        "### Problem 3b-3 What is the accuracy of this linear classifier\n",
        "\n",
        "The accuracy of a linear classifier is the proportion of correctly classified instances to the total number of instances. In this case, we can calculate the accuracy based on the decision boundary $z(x_1, x_2) = 0$. We can classify points as positive if $z(x_1, x_2) > 0$ and negative if $z(x_1, x_2) < 0$.\n",
        "\n",
        "I'm going to make a hypothetical dataset of 6 points with known labels and see how many are correctly classified by the decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Suppose we have a small dataset of 6 points\n",
        "# Each entry: (x1, x2, true_label)\n",
        "# true_label = 1 for positive, 0 for negative\n",
        "dataset = [\n",
        "    (2, 3, 1),  # point1\n",
        "    (5, 0, 0),  # point2\n",
        "    (4, 3, 1),  # point3\n",
        "    (3, 2, 1),  # point4\n",
        "    (4, 4, 1),  # point5\n",
        "    (6, 2, 0)   # point6\n",
        "]\n",
        "\n",
        "correct = 0\n",
        "for (x1, x2, y_true) in dataset:\n",
        "    z_val = decision_function(x1, x2)\n",
        "    # Predicted label: 1 if z > 0, else 0\n",
        "    y_pred = 1 if z_val > 0 else 0\n",
        "    if y_pred == y_true:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(dataset)\n",
        "print(\"Number of points:\", len(dataset))\n",
        "print(\"Correctly classified:\", correct)\n",
        "print(\"Accuracy on this small dataset:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy of the linear classifier on this small dataset is 4 out of 6, which is approximately 66.67%. This means that the classifier correctly classified 4 out of the 6 points based on the decision boundary $z(x_1, x_2) = 0$. \n",
        "\n",
        "### Problem 3b-4 If you apply the model to a test dataset and get the confusion matrix as follows:\n",
        "\n",
        ":::\n",
        "![confusion matrix dataset](images/Problem_3_b4.png){fig-align=\"center\" fig-cap=\"(a) Confusion matrix for the test dataset.\"}\n",
        ":::\n",
        "\n",
        "#### Problem 3b-4-1 Calculate the accuracy\n",
        "\n",
        "The accuracy of a classifier is defined as the ratio of the number of correct predictions to the total number of predictions. It is a measure of how often the classifier is correct. The accuracy can be calculated using the formula: $$\\frac{45 + 40}{45 + 5 + 10 + 40} = \\frac{85}{100} = 85%)$$\n",
        "\n",
        "#### Problem 3b-4-2 Calculate the precision and recall\n",
        "\n",
        "**Precision** is the ratio of true positive predictions to the total number of positive predictions made by the classifier. It is a measure of how many of the positive predictions are actually correct. The precision can be calculated using the formula: $$\\frac{45}{45 + 10} = \\frac{45}{55} = 0.818$$\n",
        "\n",
        "**Recall** is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It is a measure of how many of the actual positive instances are correctly predicted by the classifier. The recall can be calculated using the formula: $$\\frac{45}{45 + 5} = \\frac{45}{50} = 0.9$$\n",
        "\n",
        "Below is the Python code to calculate the accuracy, precision, and recall based on the given confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Confusion matrix counts\n",
        "TP = 45  # True Positives\n",
        "FN = 5   # False Negatives\n",
        "FP = 10  # False Positives\n",
        "TN = 40  # True Negatives\n",
        "\n",
        "accuracy_cm = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision_cm = TP / (TP + FP)\n",
        "recall_cm = TP / (TP + FN)\n",
        "\n",
        "print(\"Confusion Matrix Based Performance:\")\n",
        "print(\"Accuracy =\", accuracy_cm)\n",
        "print(\"Precision =\", precision_cm)\n",
        "print(\"Recall =\", recall_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read training data\n",
        "train_file = \"https://raw.githubusercontent.com/binbenliu/Teaching/main/data/Diabetes/diabetes_train.csv\"\n",
        "train_df = pd.read_csv(train_file, header='infer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read test data\n",
        "test_file = \"https://raw.githubusercontent.com/binbenliu/Teaching/main/data/Diabetes/diabetes_test.csv\"\n",
        "test_df = pd.read_csv(test_file, header='infer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# display the first few rows of the training data\n",
        "train_df.head()\n",
        "cols = train_df.columns\n",
        "cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x_cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
        "       'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
        "y_col = 'Outcome'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# train data\n",
        "X_train = train_df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
        "       'BMI', 'DiabetesPedigreeFunction', 'Age']].values\n",
        "y_train = train_df['Outcome'].values\n",
        "\n",
        "# test data\n",
        "X_test = test_df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
        "       'BMI', 'DiabetesPedigreeFunction', 'Age']].values\n",
        "y_test = test_df['Outcome'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!pip install -U scikit-learn\n",
        "## here is your code to train the decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a decision tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# here is your code to make prediciton and get the classfication metrics based on the trained decision tree model\n",
        "# Predict the labels for the test data\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the classification metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Classification Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Displaying the decision tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train a shallower tree by limiting max_depth\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(\n",
        "    dt_classifier,\n",
        "    filled=True,\n",
        "    feature_names=x_cols,\n",
        "    class_names=[\"0\", \"1\"],  # or [\"No Diabetes\", \"Diabetes\"]\n",
        "    rounded=True,\n",
        "    proportion=True,\n",
        "    precision=2\n",
        ")\n",
        "plt.title(\"Decision Tree (max_depth=3)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code above, I trained a decision tree classifier with a maximum depth of 3 and visualized the tree using the `plot_tree` function from scikit-learn. The resulting decision tree is displayed above. The issue with limiting the depth of the tree is that it may not capture all the underlying patterns in the data, leading to lower accuracy and performance. \n",
        "\n",
        "\n",
        "**Pruning the Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "path = dt_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "best_alpha = 0.0030  # smaller alpha -> more splits\n",
        "pruned_tree = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(pruned_tree, filled=True, feature_names=x_cols, class_names=[\"0\",\"1\"])\n",
        "plt.title(f\"Pruned Decision Tree (ccp_alpha={best_alpha})\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: Logistic Regression\n",
        "\n",
        "We consider a logistic regression model with **L2 regularization** by default. The decision function is:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^n l\\left(\\sigma\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b\\right),  y^{(i)}\\right) + \\frac{\\alpha}{2} \\|\\mathbf{w}\\|^2.\n",
        "$$\n",
        "\n",
        "where \\(b\\) is the intercept, \\(w_i\\) are the weights, and \\(x_i\\) are the feature values.  \n",
        "The predicted probability for the positive class (\\(y=1\\)) is given by:\n",
        "\n",
        "$$\n",
        "p(y=1 \\mid x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Create and train the Logistic Regression model\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000, \n",
        "    random_state=42\n",
        ")\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Predict on the test set\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# 3. Evaluate performance\n",
        "accuracy_lr  = accuracy_score(y_test, y_pred_lr)\n",
        "precision_lr = precision_score(y_test, y_pred_lr)\n",
        "recall_lr    = recall_score(y_test, y_pred_lr)\n",
        "f1_lr        = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Logistic Regression Metrics on Test Data\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Accuracy :  {accuracy_lr:.4f}\")\n",
        "print(f\"Precision:  {precision_lr:.4f}\")\n",
        "print(f\"Recall   :  {recall_lr:.4f}\")\n",
        "print(f\"F1 Score :  {f1_lr:.4f}\")\n",
        "\n",
        "# 4. Feature importances (coefficients)\n",
        "coef_values = lr_model.coef_[0]\n",
        "intercept = lr_model.intercept_[0]\n",
        "\n",
        "print(\"\\nLogistic Regression Feature Importances:\")\n",
        "for feature_name, coef_val in zip(x_cols, coef_values):\n",
        "    print(f\"{feature_name}: {coef_val:.4f}\")\n",
        "\n",
        "print(f\"\\nIntercept (b): {intercept:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I utilized the `LogisticRegression` class from scikit-learn to train a logistic regression model on the training data. I then used this model to predict the labels for the test data and calculated various classification metrics such as accuracy, precision, recall, and F1 score. Additionally, I extracted the feature importances (coefficients) and the intercept term from the trained logistic regression model. Now the max iterations is set to 1000 to ensure convergence. However if I use too many iterations, it may lead to **overfitting**.\n",
        "\n",
        "### LR: Predict and evaulate the following classification metrics: accuracy, precision, recall, and F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# here is your code to make prediciton and get the classfication metrics based on the trained logistic regression model\n",
        "# Predict the labels for the test data\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate the classification metrics\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "precision_lr = precision_score(y_test, y_pred_lr)\n",
        "recall_lr = recall_score(y_test, y_pred_lr)\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Logistic Regression Classification Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_lr)\n",
        "print(\"Precision:\", precision_lr)\n",
        "print(\"Recall:\", recall_lr)\n",
        "print(\"F1 Score:\", f1_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importance: Display the feature importance scores, namely $w_1, w_2, ..., w_d$, and discuss what you find from the model.\n",
        "\n",
        "The feature importance scores represent the weights assigned to each feature by the logistic regression model. These weights indicate the impact of each feature on the predicted probability of the positive class. Features with higher absolute weights have a stronger influence on the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the feature importances (coefficients) and the intercept term\n",
        "coef_values = lr_model.coef_[0]\n",
        "intercept = lr_model.intercept_[0]\n",
        "\n",
        "print(\"Logistic Regression Feature Importances:\")\n",
        "for feature_name, coef_val in zip(x_cols, coef_values):\n",
        "    print(f\"{feature_name}: {coef_val:.4f}\")\n",
        "    \n",
        "print(f\"\\nIntercept (b): {intercept:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/collinedwards/.virtualenvs/r-reticulate/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}