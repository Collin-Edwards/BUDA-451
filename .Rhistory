print(f'num train records: {len(train_df)}')
# read test data
test_file = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
test_df = pd.read_csv(test_file, header=0)
print(f'num test records: {len(test_df)}')
# Read the training data and test data
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
test_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
train_df = pd.read_csv(train_url)
test_df = pd.read_csv(test_url)
# We assume that the feature extraction (like TF-IDF) is already done.
# For simplicity, we use a placeholder feature matrix X and label vector y.
# In a real assignment, you would extract the features from the text reviews.
X_train = train_df.drop(columns=['label']).values
y_train = train_df['label'].values
X_test = test_df.drop(columns=['label']).values
y_test = test_df['label'].values
max_depths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]
print("Decision Tree Accuracy for different max_depths:")
for depth in max_depths:
clf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred = clf_tree.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"max_depth={depth}: Accuracy = {acc:.4f}")
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_LR = LogisticRegression(penalty='l2', C=regu, fit_intercept=True, max_iter=1000, random_state=42)
clf_LR.fit(X_train, y_train)
y_pred = clf_LR.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_LR = LogisticRegression(penalty='l2', C=regu, fit_intercept=True, max_iter=1000, random_state=42)
clf_LR.fit(X_train, y_train)
y_pred = clf_LR.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
```{python}
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_LR = LogisticRegression(penalty='l2', C=regu, fit_intercept=True, max_iter=1000, random_state=42)
clf_LR.fit(X_train, y_train)
y_pred = clf_LR.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
```
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_LR = LogisticRegression(penalty='l2', C=regu, fit_intercept=True, max_iter=1000, random_state=42)
clf_LR.fit(X_train, y_train)
y_pred = clf_LR.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
from sklearn.svm import SVC
print("\nSVM Accuracy for different C values:")
for regu in reguList:
clf_svm = SVC(C=regu, kernel='rbf', random_state=42)
clf_svm.fit(X_train, y_train)
y_pred = clf_svm.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
from sklearn.svm import SVC
print("\nSVM Accuracy for different C values:")
for regu in reguList:
clf_svm = SVC(C=regu, kernel='rbf', random_state=42)
clf_svm.fit(X_train, y_train)
y_pred = clf_svm.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
from sklearn.svm import SVC
print("\nSVM Accuracy for different C values:")
for regu in reguList:
clf_svm = SVC(C=regu, kernel='rbf', random_state=42)
clf_svm.fit(X_train, y_train)
y_pred = clf_svm.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
from sklearn.svm import SVC
print("\nSVM Accuracy for different C values:")
for regu in reguList:
clf_svm = SVC(C=regu, kernel='rbf', random_state=42)
clf_svm.fit(X_train, y_train)
y_pred = clf_svm.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
import pandas as pd
from sklearn import tree
from sklearn.metrics import accuracy_score
!pip install -U NLTK
import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfTransformer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy.random import random
# read training data
train_file = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
train_df = pd.read_csv(train_file, header=0)
print(f'num train records: {len(train_df)}')
# read test data
test_file = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
test_df = pd.read_csv(test_file, header=0)
print(f'num test records: {len(test_df)}')
# Read the training data and test data
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
test_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
train_df = pd.read_csv(train_url)
test_df = pd.read_csv(test_url)
# We assume that the feature extraction (like TF-IDF) is already done.
# For simplicity, we use a placeholder feature matrix X and label vector y.
# In a real assignment, you would extract the features from the text reviews.
X_train = train_df.drop(columns=['label']).values
y_train = train_df['label'].values
X_test = test_df.drop(columns=['label']).values
y_test = test_df['label'].values
max_depths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]
print("Decision Tree Accuracy for different max_depths:")
for depth in max_depths:
clf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred = clf_tree.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"max_depth={depth}: Accuracy = {acc:.4f}")
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_LR = LogisticRegression(penalty='l2', C=regu, fit_intercept=True, max_iter=1000, random_state=42)
clf_LR.fit(X_train, y_train)
y_pred = clf_LR.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C={regu}: Accuracy = {acc:.4f}")
# Read the training and test data from the provided URLs.
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
test_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
train_df = pd.read_csv(train_url)
test_df = pd.read_csv(test_url)
# Print the first few rows and the column names.
print("Train Data Head:")
print(train_df.head())
print("Columns in Train Data:")
print(train_df.columns)
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
# Read the training and test data
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
test_url  = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
train_df = pd.read_csv(train_url)
test_df  = pd.read_csv(test_url)
# Print the columns to see what they are.
print("Columns in Train Data:")
print(train_df.columns)
# Our data has two columns: y_label and text.
# We use y_label as the target and text as our input.
# Convert the text data to TF-IDF features.
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_df['text'])
X_test = vectorizer.transform(test_df['text'])
# Set y as the label.
y_train = train_df['y_label'].values
y_test  = test_df['y_label'].values
print("Number of training records:", X_train.shape[0])
print("Number of test records:", X_test.shape[0])
from sklearn import tree
from sklearn.metrics import accuracy_score
max_depths = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50]
print("Decision Tree Accuracy for different max_depths:")
for depth in max_depths:
clf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred = clf_tree.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"max_depth = {depth}: Accuracy = {acc:.4f}")
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_lr = LogisticRegression(penalty='l2', C=regu, max_iter=1000, random_state=42)
clf_lr.fit(X_train, y_train)
y_pred = clf_lr.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C = {regu}: Accuracy = {acc:.4f}")
from sklearn.svm import SVC
print("\nSVM Accuracy for different C values:")
for regu in reguList:
clf_svm = SVC(C=regu, kernel='rbf', random_state=42)
clf_svm.fit(X_train, y_train)
y_pred = clf_svm.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C = {regu}: Accuracy = {acc:.4f}")
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Use the raw URLs for the data files
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/IntroAI/data/diabetes_train.csv"
test_url  = "https://raw.githubusercontent.com/binbenliu/Teaching/main/IntroAI/data/diabetes_test.csv"
# Load the data
train_df = pd.read_csv(train_url)
test_df  = pd.read_csv(test_url)
# Define the feature columns and target column
x_cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
'BMI', 'DiabetesPedigreeFunction', 'Age']
y_col = 'Outcome'
# Create feature matrices and target vectors
X_train = train_df[x_cols].values
y_train = train_df[y_col].values
X_test = test_df[x_cols].values
y_test = test_df[y_col].values
print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)
from sklearn.tree import DecisionTreeClassifier
# Train the base decision tree model
clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred_tree = clf_tree.predict(X_test)
# Compute performance metrics for the base model
acc_tree = accuracy_score(y_test, y_pred_tree)
prec_tree = precision_score(y_test, y_pred_tree)
rec_tree = recall_score(y_test, y_pred_tree)
f1_tree = f1_score(y_test, y_pred_tree)
print("Base Decision Tree Performance:")
print(f"Accuracy:  {acc_tree:.4f}")
print(f"Precision: {prec_tree:.4f}")
print(f"Recall:    {rec_tree:.4f}")
print(f"F1 Score:  {f1_tree:.4f}")
from sklearn.ensemble import BaggingClassifier
clf_bag = BaggingClassifier(
base_estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,  # number of trees
max_samples=100,   # number of samples for each tree
bootstrap=True,    # sample with replacement
random_state=42
)
clf_bag.fit(X_train, y_train)
y_pred_bag = clf_bag.predict(X_test)
# Compute performance metrics for the Bagging model
acc_bag = accuracy_score(y_test, y_pred_bag)
prec_bag = precision_score(y_test, y_pred_bag)
rec_bag = recall_score(y_test, y_pred_bag)
f1_bag = f1_score(y_test, y_pred_bag)
print("Bagging Performance:")
print(f"Accuracy:  {acc_bag:.4f}")
print(f"Precision: {prec_bag:.4f}")
print(f"Recall:    {rec_bag:.4f}")
print(f"F1 Score:  {f1_bag:.4f}")
from sklearn.tree import DecisionTreeClassifier
# Train the base decision tree model
clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred_tree = clf_tree.predict(X_test)
# Compute performance metrics for the base model
acc_tree = accuracy_score(y_test, y_pred_tree)
prec_tree = precision_score(y_test, y_pred_tree)
rec_tree = recall_score(y_test, y_pred_tree)
f1_tree = f1_score(y_test, y_pred_tree)
print("Base Decision Tree Performance:")
print(f"Accuracy:  {acc_tree:.4f}")
print(f"Precision: {prec_tree:.4f}")
print(f"Recall:    {rec_tree:.4f}")
print(f"F1 Score:  {f1_tree:.4f}")
from sklearn.ensemble import BaggingClassifier
clf_bag = BaggingClassifier(
base_estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,  # number of trees
max_samples=100,   # number of samples for each tree
bootstrap=True,    # sample with replacement
random_state=42
)
clf_bag.fit(X_train, y_train)
y_pred_bag = clf_bag.predict(X_test)
# Compute performance metrics for the Bagging model
acc_bag = accuracy_score(y_test, y_pred_bag)
prec_bag = precision_score(y_test, y_pred_bag)
rec_bag = recall_score(y_test, y_pred_bag)
f1_bag = f1_score(y_test, y_pred_bag)
print("Bagging Performance:")
print(f"Accuracy:  {acc_bag:.4f}")
print(f"Precision: {prec_bag:.4f}")
print(f"Recall:    {rec_bag:.4f}")
print(f"F1 Score:  {f1_bag:.4f}")
from sklearn.ensemble import BaggingClassifier
# Use "estimator" instead of "base_estimator" for recent versions of scikit-learn
clf_bag = BaggingClassifier(
estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,  # number of trees
max_samples=100,   # number of samples for each tree
bootstrap=True,    # sample with replacement
random_state=42
)
clf_bag.fit(X_train, y_train)
y_pred_bag = clf_bag.predict(X_test)
# Compute performance metrics for the Bagging model
acc_bag = accuracy_score(y_test, y_pred_bag)
prec_bag = precision_score(y_test, y_pred_bag)
rec_bag = recall_score(y_test, y_pred_bag)
f1_bag = f1_score(y_test, y_pred_bag)
print("Bagging Performance:")
print(f"Accuracy:  {acc_bag:.4f}")
print(f"Precision: {prec_bag:.4f}")
print(f"Recall:    {rec_bag:.4f}")
print(f"F1 Score:  {f1_bag:.4f}")
from sklearn.ensemble import RandomForestClassifier
clf_rf = RandomForestClassifier(
criterion='entropy', max_depth=4, n_estimators=500, random_state=42
)
clf_rf.fit(X_train, y_train)
y_pred_rf = clf_rf.predict(X_test)
# Compute performance metrics for the Random Forest model
acc_rf = accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
rec_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
print("Random Forest Performance:")
print(f"Accuracy:  {acc_rf:.4f}")
print(f"Precision: {prec_rf:.4f}")
print(f"Recall:    {rec_rf:.4f}")
print(f"F1 Score:  {f1_rf:.4f}")
from sklearn.ensemble import AdaBoostClassifier
clf_adaboost = AdaBoostClassifier(
base_estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,
random_state=42
)
clf_adaboost.fit(X_train, y_train)
y_pred_adaboost = clf_adaboost.predict(X_test)
# Compute performance metrics for AdaBoost
acc_adaboost = accuracy_score(y_test, y_pred_adaboost)
prec_adaboost = precision_score(y_test, y_pred_adaboost)
rec_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
print("AdaBoost Performance:")
print(f"Accuracy:  {acc_adaboost:.4f}")
print(f"Precision: {prec_adaboost:.4f}")
print(f"Recall:    {rec_adaboost:.4f}")
print(f"F1 Score:  {f1_adaboost:.4f}")
from sklearn.ensemble import AdaBoostClassifier
clf_adaboost = AdaBoostClassifier(
estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,
random_state=42
)
clf_adaboost.fit(X_train, y_train)
y_pred_adaboost = clf_adaboost.predict(X_test)
# Compute performance metrics for AdaBoost
acc_adaboost = accuracy_score(y_test, y_pred_adaboost)
prec_adaboost = precision_score(y_test, y_pred_adaboost)
rec_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
print("AdaBoost Performance:")
print(f"Accuracy:  {acc_adaboost:.4f}")
print(f"Precision: {prec_adaboost:.4f}")
print(f"Recall:    {rec_adaboost:.4f}")
print(f"F1 Score:  {f1_adaboost:.4f}")
# Read the training and test data from the provided URLs.
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
test_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
train_df = pd.read_csv(train_url)
test_df = pd.read_csv(test_url)
# Print the first few rows and the column names.
print("Train Data Head:")
print(train_df.head())
print("Columns in Train Data:")
print(train_df.columns)
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
# Read the training and test data
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/train.csv"
test_url  = "https://raw.githubusercontent.com/binbenliu/Teaching/main/data/cornell_movie/test.csv"
train_df = pd.read_csv(train_url)
test_df  = pd.read_csv(test_url)
# Print the columns to see what they are.
print("Columns in Train Data:")
print(train_df.columns)
# Our data has two columns: y_label and text.
# I use y_label as the target and text as our input.
# Convert the text data to TF-IDF features.
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_df['text'])
X_test = vectorizer.transform(test_df['text'])
# Set y as the label.
y_train = train_df['y_label'].values
y_test  = test_df['y_label'].values
print("Number of training records:", X_train.shape[0])
print("Number of test records:", X_test.shape[0])
from sklearn import tree
from sklearn.metrics import accuracy_score
max_depths = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50]
print("Decision Tree Accuracy for different max_depths:")
for depth in max_depths:
clf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred = clf_tree.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"max_depth = {depth}: Accuracy = {acc:.4f}")
from sklearn.linear_model import LogisticRegression
reguList = [0.1, 0.5, 1.0, 5, 10, 20, 50, 100]
print("\nLogistic Regression Accuracy for different C values:")
for regu in reguList:
clf_lr = LogisticRegression(penalty='l2', C=regu, max_iter=1000, random_state=42)
clf_lr.fit(X_train, y_train)
y_pred = clf_lr.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C = {regu}: Accuracy = {acc:.4f}")
from sklearn.svm import SVC
print("\nSVM Accuracy for different C values:")
for regu in reguList:
clf_svm = SVC(C=regu, kernel='rbf', random_state=42)
clf_svm.fit(X_train, y_train)
y_pred = clf_svm.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"C = {regu}: Accuracy = {acc:.4f}")
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Use the raw URLs for the data files
train_url = "https://raw.githubusercontent.com/binbenliu/Teaching/main/IntroAI/data/diabetes_train.csv"
test_url  = "https://raw.githubusercontent.com/binbenliu/Teaching/main/IntroAI/data/diabetes_test.csv"
# Load the data
train_df = pd.read_csv(train_url)
test_df  = pd.read_csv(test_url)
# Define the feature columns and target column
x_cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
'BMI', 'DiabetesPedigreeFunction', 'Age']
y_col = 'Outcome'
# Create feature matrices and target vectors
X_train = train_df[x_cols].values
y_train = train_df[y_col].values
X_test = test_df[x_cols].values
y_test = test_df[y_col].values
print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)
from sklearn.tree import DecisionTreeClassifier
# Train the base decision tree model
clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)
clf_tree.fit(X_train, y_train)
y_pred_tree = clf_tree.predict(X_test)
# Compute performance metrics for the base model
acc_tree = accuracy_score(y_test, y_pred_tree)
prec_tree = precision_score(y_test, y_pred_tree)
rec_tree = recall_score(y_test, y_pred_tree)
f1_tree = f1_score(y_test, y_pred_tree)
print("Base Decision Tree Performance:")
print(f"Accuracy:  {acc_tree:.4f}")
print(f"Precision: {prec_tree:.4f}")
print(f"Recall:    {rec_tree:.4f}")
print(f"F1 Score:  {f1_tree:.4f}")
from sklearn.ensemble import BaggingClassifier
clf_bag = BaggingClassifier(
estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,  # number of trees
max_samples=100,   # number of samples for each tree
bootstrap=True,    # sample with replacement
random_state=42
)
clf_bag.fit(X_train, y_train)
y_pred_bag = clf_bag.predict(X_test)
# Compute performance metrics for the Bagging model
acc_bag = accuracy_score(y_test, y_pred_bag)
prec_bag = precision_score(y_test, y_pred_bag)
rec_bag = recall_score(y_test, y_pred_bag)
f1_bag = f1_score(y_test, y_pred_bag)
print("Bagging Performance:")
print(f"Accuracy:  {acc_bag:.4f}")
print(f"Precision: {prec_bag:.4f}")
print(f"Recall:    {rec_bag:.4f}")
print(f"F1 Score:  {f1_bag:.4f}")
from sklearn.ensemble import RandomForestClassifier
clf_rf = RandomForestClassifier(
criterion='entropy', max_depth=4, n_estimators=500, random_state=42
)
clf_rf.fit(X_train, y_train)
y_pred_rf = clf_rf.predict(X_test)
# Compute performance metrics for the Random Forest model
acc_rf = accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
rec_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
print("Random Forest Performance:")
print(f"Accuracy:  {acc_rf:.4f}")
print(f"Precision: {prec_rf:.4f}")
print(f"Recall:    {rec_rf:.4f}")
print(f"F1 Score:  {f1_rf:.4f}")
from sklearn.ensemble import AdaBoostClassifier
clf_adaboost = AdaBoostClassifier(
estimator=DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42),
n_estimators=500,
random_state=42
)
clf_adaboost.fit(X_train, y_train)
y_pred_adaboost = clf_adaboost.predict(X_test)
# Compute performance metrics for AdaBoost
acc_adaboost = accuracy_score(y_test, y_pred_adaboost)
prec_adaboost = precision_score(y_test, y_pred_adaboost)
rec_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
print("AdaBoost Performance:")
print(f"Accuracy:  {acc_adaboost:.4f}")
print(f"Precision: {prec_adaboost:.4f}")
print(f"Recall:    {rec_adaboost:.4f}")
print(f"F1 Score:  {f1_adaboost:.4f}")
