<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Collin Edwards">
<meta name="dcterms.date" content="2025-03-15">

<title>Assignment 2 BUDA 451</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Assignment2_BUDA451_files/libs/clipboard/clipboard.min.js"></script>
<script src="Assignment2_BUDA451_files/libs/quarto-html/quarto.js"></script>
<script src="Assignment2_BUDA451_files/libs/quarto-html/popper.min.js"></script>
<script src="Assignment2_BUDA451_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Assignment2_BUDA451_files/libs/quarto-html/anchor.min.js"></script>
<link href="Assignment2_BUDA451_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Assignment2_BUDA451_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Assignment2_BUDA451_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Assignment2_BUDA451_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Assignment2_BUDA451_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Assignment 2 BUDA 451</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Collin Edwards </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="data-processing-model-overfitting-validation-and-evaluation" class="level1">
<h1>Data processing, Model overfitting, Validation and Evaluation</h1>
<section id="problem-1" class="level2">
<h2 class="anchored" data-anchor-id="problem-1">Problem 1</h2>
<section id="problem-1a" class="level3">
<h3 class="anchored" data-anchor-id="problem-1a">Problem 1a</h3>
<section id="what-is-the-difference-between-dimensional-reduction-and-feature-subset-selection" class="level4">
<h4 class="anchored" data-anchor-id="what-is-the-difference-between-dimensional-reduction-and-feature-subset-selection">What is the difference between dimensional reduction and feature subset selection?</h4>
<dl>
<dt><strong>Dimensionality reduction</strong></dt>
<dd>
<p>This technique transforms the original high-dimensional feature space into a lower-dimensional space. It does this by creating new features that are combinations of the original features that captures most of the variance . For example, <em>Principal Component Analysis (PCA)</em> is a common method used for dimensionality reduction.</p>
</dd>
<dt><strong>Feature subset selection</strong></dt>
<dd>
<p>For this approach it selects a subset of the original features without transforming them. The goal is to identify and retain the most relevant and informative features while discarding the less important ones. This can be done using methods like <em>forward selection</em>, <em>backward elimination</em>, or <em>recursive feature elimination</em>.</p>
</dd>
</dl>
<p><mark>The main difference is that dimensionality reduction creates new features, while feature subset selection retains the original features.</mark></p>
</section>
</section>
<section id="problem-1b" class="level3">
<h3 class="anchored" data-anchor-id="problem-1b">Problem 1b</h3>
<section id="what-are-the-possible-reasons-for-model-overfitting" class="level4">
<h4 class="anchored" data-anchor-id="what-are-the-possible-reasons-for-model-overfitting">What are the possible reasons for model overfitting?</h4>
<ol type="1">
<li><strong>Complex Models</strong>: Using a model that is too complex for the amount of data available can lead to overfitting. For example, a deep neural network with many layers may learn to memorize the training data instead of generalizing from it.</li>
<li><strong>Insufficient Data</strong>: When the training dataset is too small, the model may learn noise and outliers instead of the underlying patterns.</li>
<li><strong>Lack of Regularization</strong>: Regularization techniques (like L1 or L2 regularization) help to penalize complex models. Without them, the model may fit the training data too closely.</li>
<li><strong>Noisy Data</strong>: If the training data contains a lot of noise or outliers, the model may learn to fit these anomalies rather than the true underlying patterns.</li>
<li><strong>Inadequate Cross-Validation</strong>: If the model is not properly validated using techniques like k-fold cross-validation, it may perform well on the training data but poorly on unseen data.</li>
</ol>
</section>
</section>
<section id="problem-1c" class="level3">
<h3 class="anchored" data-anchor-id="problem-1c">Problem 1c</h3>
<section id="what-can-you-do-to-avoid-overfitting-elaborate-you-answers-with-decision-trees-and-logistic-regression." class="level4">
<h4 class="anchored" data-anchor-id="what-can-you-do-to-avoid-overfitting-elaborate-you-answers-with-decision-trees-and-logistic-regression.">What can you do to avoid overfitting? Elaborate you answers with <em>Decision Trees</em>, and <em>Logistic Regression</em>.</h4>
<ol type="1">
<li><p><strong>Decision Trees</strong>: To avoid overfitting in decision trees, you can use techniques such as: Pruning<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, setting a maximum depth<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, requiring a minimum number of samples per leaf, or enforcing a minimum impurity decrease to restrict tree complexity.</p></li>
<li><p><strong>Logistic Regression</strong>: To avoid overfitting in logistic regression, you can:</p>
<ul>
<li>Use regularization techniques such as L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients.</li>
<li>Feature selection to remove irrelevant or redundant features.</li>
<li>Cross-validation to ensure that the model generalizes well to unseen data.</li>
</ul></li>
</ol>
</section>
</section>
<section id="problem-1d" class="level3">
<h3 class="anchored" data-anchor-id="problem-1d">Problem 1d</h3>
<section id="whats-the-difference-between-validation-dataset-and-test-dataset" class="level4">
<h4 class="anchored" data-anchor-id="whats-the-difference-between-validation-dataset-and-test-dataset">What’s the difference between validation dataset and test dataset?</h4>
<p><strong>Validation Dataset</strong>: This dataset is used during the training process to tune the model’s hyperparameters and make decisions about the model architecture. It helps in selecting the best model among different candidates. The validation set is not used for training but is used to evaluate the model’s performance during training.</p>
<p><strong>Test Dataset</strong>: This dataset is used to evaluate the final performance of the model after it has been trained and validated. It provides an unbiased estimate of the model’s performance on unseen data. The test set should not be used in any way during the training or validation process.</p>
</section>
</section>
<section id="problem-1e" class="level3">
<h3 class="anchored" data-anchor-id="problem-1e">Problem 1e</h3>
<section id="describe-10-fold-cross-validation.-whats-the-advantages-and-disadvantages-as-compared-to-one-train-test-split-in-model-evolution" class="level4">
<h4 class="anchored" data-anchor-id="describe-10-fold-cross-validation.-whats-the-advantages-and-disadvantages-as-compared-to-one-train-test-split-in-model-evolution">Describe 10 fold-cross validation. What’s the advantage(s) and disadvantage(s) as compared to one train-test split in model evolution?</h4>
<p><strong>10-Fold Cross-Validation</strong>: In this method, the dataset is divided into 10 equal parts (or folds). The model is trained on 9 folds and validated on the remaining fold. This process is repeated 10 times, with each fold being used as the validation set once. The final performance metric is usually the average of the performance across all 10 folds.</p>
<p><strong>Advantages</strong>: 1. <strong>More Reliable Estimate</strong>: It provides a more reliable estimate of the model’s performance since it uses multiple train-test splits. 2. <strong>Better Use of Data</strong>: All data points are used for both training and validation, which is especially useful when the dataset is small. 3. <strong>Reduces Variance</strong>: It reduces the variance associated with a single train-test split, making the performance estimate more stable.</p>
<p><strong>Disadvantages</strong>: 1. <strong>Computationally Expensive</strong>: It requires more computational resources and time since the model is trained multiple times. 2. <strong>Complexity</strong>: It adds complexity to the model evaluation process, making it harder to implement and interpret.</p>
</section>
</section>
<section id="problem-1f" class="level3">
<h3 class="anchored" data-anchor-id="problem-1f">Problem 1f</h3>
<section id="why-can-accuracy-be-a-bad-metric-to-evaluate-your-classifier-what-other-metrics-you-can-use" class="level4">
<h4 class="anchored" data-anchor-id="why-can-accuracy-be-a-bad-metric-to-evaluate-your-classifier-what-other-metrics-you-can-use">Why can accuracy be a bad metric to evaluate your classifier? What other metrics you can use?</h4>
<dl>
<dt><strong>Issues with Accuracy</strong></dt>
<dd>
<p>Accuracy can be misleading, especially in imbalanced datasets where one class significantly outnumbers the other. For example, if 95% of the data belongs to one class, a model that predicts the majority class will have 95% accuracy but will not be useful for predicting the minority class. It doesnt show how many false positives or false negatives there are. <strong>Other Metrics</strong>: 1. <strong>Precision</strong>: Measures the proportion of true positive predictions among all positive predictions. It is useful when the cost of false positives is high. 2. <strong>Recall (Sensitivity)</strong>: Measures the proportion of true positive predictions among all actual positive instances. It is useful when the cost of false negatives is high. 3. <strong>F1 Score</strong>: The harmonic mean of precision and recall, providing a balance between the two metrics. 4. <strong>ROC-AUC</strong>: The area under the receiver operating characteristic curve, which provides a measure of the model’s ability to distinguish between classes.</p>
</dd>
</dl>
</section>
</section>
</section>
<section id="problem-2-decision-trees" class="level2">
<h2 class="anchored" data-anchor-id="problem-2-decision-trees">Problem 2 Decision Trees</h2>
<p>Consider the training examples shown bellow for a binary classification problem.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Movie ID</th>
<th>Format</th>
<th>Movie Category</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td>DVD</td>
<td>Entertainment</td>
<td>C0</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td>DVD</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td>DVD</td>
<td>Documentaries</td>
<td>C0</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td>DVD</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5</td>
<td>DVD</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="even">
<td style="text-align: left;">6</td>
<td>DVD</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">7</td>
<td>Online</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="even">
<td style="text-align: left;">8</td>
<td>Online</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">9</td>
<td>Online</td>
<td>Comedy</td>
<td>C0</td>
</tr>
<tr class="even">
<td style="text-align: left;">10</td>
<td>Online</td>
<td>Documentaries</td>
<td>C0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">11</td>
<td>DVD</td>
<td>Comedy</td>
<td>C1</td>
</tr>
<tr class="even">
<td style="text-align: left;">12</td>
<td>DVD</td>
<td>Entertainment</td>
<td>C1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">13</td>
<td>Online</td>
<td>Entertainment</td>
<td>C1</td>
</tr>
<tr class="even">
<td style="text-align: left;">14</td>
<td>Online</td>
<td>Documentaries</td>
<td>C1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">15</td>
<td>Online</td>
<td>Documentaries</td>
<td>C1</td>
</tr>
<tr class="even">
<td style="text-align: left;">16</td>
<td>Online</td>
<td>Documentaries</td>
<td>C1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">17</td>
<td>Online</td>
<td>Documentaries</td>
<td>C1</td>
</tr>
<tr class="even">
<td style="text-align: left;">18</td>
<td>Online</td>
<td>Entertainment</td>
<td>C1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">19</td>
<td>Online</td>
<td>Documentaries</td>
<td>C1</td>
</tr>
<tr class="even">
<td style="text-align: left;">20</td>
<td>Online</td>
<td>Documentaries</td>
<td>C1</td>
</tr>
</tbody>
</table>
<section id="problem-2a-compute-the-entropy-for-the-movie-id-attribute." class="level4">
<h4 class="anchored" data-anchor-id="problem-2a-compute-the-entropy-for-the-movie-id-attribute.">Problem 2a Compute the Entropy for the <strong>Movie ID</strong> attribute.</h4>
<p>Entropy is given by the formula:</p>
<p><span class="math display">\[Entropy = - \sum_{i=0}^{C-1} p_i(t) \log_2 p_i(t)\]</span></p>
<p>where <span class="math inline">\(p_i(t)\)</span> is the proportion of the <span class="math inline">\(i\)</span>-th class in the node <span class="math inline">\(t\)</span> and <span class="math inline">\(C\)</span> is the number of classes. Below we define a helper function in Python and compute the overall entropy.</p>
<div id="3d2b0097" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(counts):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> np.array(counts)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> counts.<span class="bu">sum</span>()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    probabilities <span class="op">=</span> counts <span class="op">/</span> total</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    probabilities <span class="op">=</span> probabilities[probabilities <span class="op">&gt;</span> <span class="dv">0</span>]  <span class="co"># avoid log2(0)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(probabilities <span class="op">*</span> np.log2(probabilities))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Overall dataset: 10 of C0 and 10 of C1</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>overall_entropy <span class="op">=</span> entropy([<span class="dv">10</span>, <span class="dv">10</span>])</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Overall Entropy:"</span>, overall_entropy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overall Entropy: 1.0</code></pre>
</div>
</div>
<p>In this case, we have two classes: <span class="math inline">\(C0\)</span> and <span class="math inline">\(C1\)</span>. The entropy for the <strong>Movie ID</strong> attribute can be calculated as follows:</p>
<div id="e214adfe" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>movieid_entropy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Movie ID Entropy:"</span>, movieid_entropy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Movie ID Entropy: 0</code></pre>
</div>
</div>
<p>Each movie ID is unique, so the entropy for the <strong>Movie ID</strong> attribute is 0.</p>
</section>
<section id="problem-2b-compute-the-entropy-for-the-format-attribute." class="level4">
<h4 class="anchored" data-anchor-id="problem-2b-compute-the-entropy-for-the-format-attribute.">Problem 2b Compute the Entropy for the <strong>Format</strong> attribute.</h4>
<p>We calculate the entropy for each group and then compute the weighted average to get the overall entropy.</p>
<div id="6e7936bd" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entropy for DVD group</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dvd_entropy <span class="op">=</span> entropy([<span class="dv">6</span>, <span class="dv">2</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entropy for Online group</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>online_entropy <span class="op">=</span> entropy([<span class="dv">4</span>, <span class="dv">8</span>])</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Weighted entropy for Format attribute</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>format_entropy <span class="op">=</span> (<span class="dv">8</span><span class="op">/</span><span class="dv">20</span>) <span class="op">*</span> dvd_entropy <span class="op">+</span> (<span class="dv">12</span><span class="op">/</span><span class="dv">20</span>) <span class="op">*</span> online_entropy</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DVD Entropy:"</span>, dvd_entropy)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Online Entropy:"</span>, online_entropy)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Format Weighted Entropy:"</span>, format_entropy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>DVD Entropy: 0.8112781244591328
Online Entropy: 0.9182958340544896
Format Weighted Entropy: 0.8754887502163469</code></pre>
</div>
</div>
</section>
<section id="problem-2c-compute-the-entropy-for-the-movie-category-attribute-using-multiway-split." class="level4">
<h4 class="anchored" data-anchor-id="problem-2c-compute-the-entropy-for-the-movie-category-attribute-using-multiway-split.">Problem 2c Compute the Entropy for the <strong>Movie Category</strong> attribute using multiway split.</h4>
<div id="d9263097" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entropy for Entertainment</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>entertainment_entropy <span class="op">=</span> entropy([<span class="dv">2</span>, <span class="dv">2</span>]) <span class="co"># 4 Examples of it</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entropy for Comedy</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>comedy_entropy <span class="op">=</span> entropy([<span class="dv">6</span>, <span class="dv">1</span>]) <span class="co"># 7 Examples of it</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Entropy for Documentaries</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>documentaries_entropy <span class="op">=</span> entropy([<span class="dv">2</span>, <span class="dv">7</span>]) <span class="co"># 9 Examples of it</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Weighted entropy for Movie Category attribute</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>movie_category_entropy <span class="op">=</span> (<span class="dv">4</span><span class="op">/</span><span class="dv">20</span>) <span class="op">*</span> entertainment_entropy <span class="op">+</span> (<span class="dv">7</span><span class="op">/</span><span class="dv">20</span>) <span class="op">*</span> comedy_entropy <span class="op">+</span> (<span class="dv">9</span><span class="op">/</span><span class="dv">20</span>) <span class="op">*</span> documentaries_entropy</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entertainment Entropy:"</span>, entertainment_entropy)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Comedy Entropy:"</span>, comedy_entropy)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Documentaries Entropy:"</span>, documentaries_entropy)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Movie Category Weighted Entropy:"</span>, movie_category_entropy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Entertainment Entropy: 1.0
Comedy Entropy: 0.5916727785823275
Documentaries Entropy: 0.7642045065086203
Movie Category Weighted Entropy: 0.7509775004326937</code></pre>
</div>
</div>
</section>
<section id="problem-2d-which-of-the-three-attributes-has-the-lowest-entropy" class="level4">
<h4 class="anchored" data-anchor-id="problem-2d-which-of-the-three-attributes-has-the-lowest-entropy">Problem 2d Which of the three attributes has the lowest entropy?</h4>
<p>The attribute with the lowest entropy is the one that provides the most information gain when used for splitting the data. In this case, the attribute with the lowest entropy is the one that has the highest purity when the data is split based on its values. The attribute with the lowest entropy is the one that has the highest information gain when used for splitting the data.</p>
<p>In this case, the <strong>Movie Category</strong> attribute has the lowest entropy since it has near 0.75 bits, which means it provides the most information gain when used for splitting the data.</p>
</section>
<section id="problem-2e-which-of-the-three-attributes-will-you-use-for-splitting-the-root-node-briefly-explain-your-choice." class="level4">
<h4 class="anchored" data-anchor-id="problem-2e-which-of-the-three-attributes-will-you-use-for-splitting-the-root-node-briefly-explain-your-choice.">Problem 2e Which of the three attributes will you use for splitting the root node? Briefly explain your choice.</h4>
<p>The attribute that we should use for splitting the root node is the one that provides the most information gain. In this case, the <strong>Movie Category</strong> attribute has the lowest entropy, which means it provides the most information gain when used for splitting the data. By splitting the root node based on the <strong>Movie Category</strong> attribute, we can create child nodes that are more homogeneous and have higher purity.</p>
</section>
<section id="problem-2f-decision-tree-evaulation.-whats-the-accuracy-of-the-decision-tree-model-on-the-test-data" class="level4">
<h4 class="anchored" data-anchor-id="problem-2f-decision-tree-evaulation.-whats-the-accuracy-of-the-decision-tree-model-on-the-test-data">Problem 2f Decision tree evaulation. What’s the accuracy of the decision tree model on the test data?</h4>
<p>::: <img src="images/decision_tree.png" class="img-fluid quarto-figure quarto-figure-center" data-fig-cap="(a) A decision tree and (b) test dataset." alt="Decision Tree and Test Dataset"> :::</p>
<p>The decision tree model shown in the figure above is used to classify the test dataset. The test dataset consists of the following examples:### Decision Tree Structure</p>
<ol type="1">
<li><strong>Root Node (A)</strong>
<ul>
<li>If A = 0, go to node <strong>B</strong><br>
</li>
<li>If A = 1, go to node <strong>C</strong></li>
</ul></li>
<li><strong>Node B</strong>
<ul>
<li>If B = 0, predict <strong>+</strong><br>
</li>
<li>If B = 1, predict <strong>–</strong></li>
</ul></li>
<li><strong>Node C</strong>
<ul>
<li>If C = 0, predict <strong>+</strong><br>
</li>
<li>If C = 1, predict <strong>–</strong></li>
</ul></li>
</ol>
<section id="classification-of-each-instance" class="level5">
<h5 class="anchored" data-anchor-id="classification-of-each-instance">Classification of Each Instance</h5>
<ol type="1">
<li><strong>Instance 1</strong>: (A=1, B=0, C=0, Class=+)
<ul>
<li>Path: A=1 → node C; C=0 → predict +<br>
</li>
<li>Actual class is + → <strong>Correct</strong></li>
</ul></li>
<li><strong>Instance 2</strong>: (A=0, B=1, C=1, Class=+)
<ul>
<li>Path: A=0 → node B; B=1 → predict –<br>
</li>
<li>Actual class is + → <strong>Incorrect</strong></li>
</ul></li>
<li><strong>Instance 3</strong>: (A=1, B=1, C=0, Class=+)
<ul>
<li>Path: A=1 → node C; C=0 → predict +<br>
</li>
<li>Actual class is + → <strong>Correct</strong></li>
</ul></li>
<li><strong>Instance 4</strong>: (A=1, B=0, C=1, Class=–)
<ul>
<li>Path: A=1 → node C; C=1 → predict –<br>
</li>
<li>Actual class is – → <strong>Correct</strong></li>
</ul></li>
<li><strong>Instance 5</strong>: (A=1, B=0, C=0, Class=+)
<ul>
<li>Path: A=1 → node C; C=0 → predict +<br>
</li>
<li>Actual class is + → <strong>Correct</strong></li>
</ul></li>
</ol>
</section>
</section>
<section id="f-accuracy-of-the-decision-tree-model-on-the-test-data" class="level3">
<h3 class="anchored" data-anchor-id="f-accuracy-of-the-decision-tree-model-on-the-test-data">(f) Accuracy of the Decision Tree Model on the Test Data</h3>
<p>Out of 5 instances:</p>
<ul>
<li><strong>4 are correctly classified</strong> (Instances 1, 3, 4, and 5).</li>
<li><strong>1 is misclassified</strong> (Instance 2).</li>
</ul>
<p>Thus, the accuracy is:</p>
<p><span class="math display">\[\text{Accuracy} = \frac{\text{Number of Correct Classifications}}{\text{Total Number of Instances}}
= \frac{4}{5} = 0.80 = 80\%\]</span></p>
<p>Hence, the <strong>accuracy of the decision tree on the test set is 80%</strong>.</p>
</section>
</section>
<section id="problem-3-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="problem-3-linear-models">Problem 3 Linear models</h2>
<section id="problem-3a-what-are-the-parameters-for-a-generic-logistic-model-as-follows" class="level3">
<h3 class="anchored" data-anchor-id="problem-3a-what-are-the-parameters-for-a-generic-logistic-model-as-follows">Problem 3a What are the parameters for a generic logistic model as follows?</h3>
<p><span class="math display">\[z = b + w_1 x_1 + w_2 x_2 + \dots + w_d x_d, \quad p(y=1 \mid x) = \sigma(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<p>The parameters for a generic logistic model are:</p>
<ul>
<li><strong>Bias Term (<span class="math inline">\(b\)</span>)</strong>: The bias term is the intercept of the model and represents the value of the output when all input features are zero.</li>
<li><strong>Weights (<span class="math inline">\(w_1, w_2, \dots, w_d\)</span>)</strong>: The weights are the coefficients associated with each input feature <span class="math inline">\(x_1, x_2, \dots, x_d\)</span>. They determine the impact of each feature on the output.</li>
<li><strong>Logistic Function (<span class="math inline">\(\sigma(z)\)</span>)</strong>: The logistic function is used to map the linear combination of input features and weights to a probability value between 0 and 1. It is defined as <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>.</li>
</ul>
</section>
<section id="problem-3b-consider-the-following-15-points-in-a-two-dimensional-feature-space-x1x2-with-labels-as-indicated-green-is-positive-red-is-negative-in-figure-1.-lets-assume-that-after-training-the-logistic-regression-model-you-get-the-decision-function-as-follows" class="level3">
<h3 class="anchored" data-anchor-id="problem-3b-consider-the-following-15-points-in-a-two-dimensional-feature-space-x1x2-with-labels-as-indicated-green-is-positive-red-is-negative-in-figure-1.-lets-assume-that-after-training-the-logistic-regression-model-you-get-the-decision-function-as-follows">Problem 3b Consider the following 15 points in a two-dimensional feature space (x1,x2) with labels as indicated (green + is positive, red ×is negative) in Figure 1. Let’s assume that after training the logistic regression model, you get the decision function as follows:</h3>
<p><span class="math display">\[z(x_1,x_2) = 4x_1+6x_2-24\]</span></p>
<p>::: <img src="images/Assignment2_Figure_1.png" class="img-fluid quarto-figure quarto-figure-center" data-fig-cap="(a) Plots of 15 points in a two-dimensional feature space." alt="two-dimensional feature space plot"> :::</p>
<section id="problem-3b-1-what-is-the-zvalue-of-the-decision-function-for-point-a-and-whats-the-probability-of-point-as-a-positive-example" class="level4">
<h4 class="anchored" data-anchor-id="problem-3b-1-what-is-the-zvalue-of-the-decision-function-for-point-a-and-whats-the-probability-of-point-as-a-positive-example">Problem 3b-1 What is the zvalue of the decision function for point A? And what’s the probability of point as a positive example?</h4>
<div id="7bdc564e" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define the logistic regression decision function</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decision_function(x1, x2, b<span class="op">=-</span><span class="dv">24</span>, w1<span class="op">=</span><span class="dv">4</span>, w2<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns z = b + w1*x1 + w2*x2</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> b <span class="op">+</span> w1<span class="op">*</span>x1 <span class="op">+</span> w2<span class="op">*</span>x2</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Sigmoid function: 1 / (1 + e^(-z))</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> math.exp(<span class="op">-</span>z))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># (b-1) Compute z and probability for point A</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's assume point A has coordinates (x1=3, x2=3) as an example</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>xA1, xA2 <span class="op">=</span> <span class="dv">3</span>, <span class="dv">3</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>zA <span class="op">=</span> decision_function(xA1, xA2)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>pA <span class="op">=</span> sigmoid(zA)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Point A ="</span>, (xA1, xA2))</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"z(A) ="</span>, zA)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probability(A is positive) ="</span>, pA)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Point A = (3, 3)
z(A) = 6
Probability(A is positive) = 0.9975273768433653</code></pre>
</div>
</div>
<p>We define a decision function that essentially calculates the value of <span class="math inline">\(z\)</span> for a given set of input features <span class="math inline">\((x_1, x_2)\)</span>. We then use this function to compute the value of <span class="math inline">\(z\)</span> for point A, which has coordinates <span class="math inline">\((3, 3)\)</span>. Finally, we apply the sigmoid function to <span class="math inline">\(z\)</span> to obtain the probability of point A being classified as a positive example.</p>
</section>
<section id="problem-3b-2-in-the-figure-above-draw-the-decision-boundary-zx1x2-0-and-indicate-the-positive-and-negative-regions-half-planes." class="level4">
<h4 class="anchored" data-anchor-id="problem-3b-2-in-the-figure-above-draw-the-decision-boundary-zx1x2-0-and-indicate-the-positive-and-negative-regions-half-planes.">Problem 3b-2 In the figure above, draw the decision boundary, z(x1,x2) = 0, and indicate the positive and negative regions (half-planes).</h4>
<p>The decision boundary is the line where the decision function <span class="math inline">\(z(x_1, x_2) = 0\)</span>. This boundary separates the feature space into two regions: one where <span class="math inline">\(z &gt; 0\)</span> (positive region) and one where <span class="math inline">\(z &lt; 0\)</span> (negative region). The decision boundary is the set of points where the probability of being positive is equal to the probability of being negative.</p>
<p><span class="math display">\[4x_1 + 6x_2 - 24 = 0 \quad\Longrightarrow\quad 6x_2 = 24 - 4x_1 \quad\Longrightarrow\quad x_2 = 4 - \frac{2}{3} x_1.\]</span></p>
<div id="a65d913d" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the decision boundary: z(x1, x2) = 0</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> <span class="dv">4</span> <span class="op">-</span> (<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span>x1</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Example points with known labels</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>data_points <span class="op">=</span> [</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>),  <span class="co"># label=1 means positive</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">5</span>, <span class="fl">0.3</span>, <span class="dv">0</span>),  <span class="co"># label=0 means negative</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">1</span>),</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">## Re-create the boundary plot</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>plt.plot(x1, x2, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">"Decision Boundary: z=0"</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x1, x2, <span class="dv">10</span>, color<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'Predicted Positive (z&gt;0)'</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x1, x2, <span class="dv">0</span>, color<span class="op">=</span><span class="st">'lightcoral'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'Predicted Negative (z&lt;0)'</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot data points</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (px1, px2, label) <span class="kw">in</span> data_points:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        plt.plot(px1, px2, <span class="st">'go'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">"Positive"</span> <span class="cf">if</span> <span class="st">'Pos'</span> <span class="kw">not</span> <span class="kw">in</span> plt.gca().get_legend_handles_labels()[<span class="dv">1</span>] <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        plt.plot(px1, px2, <span class="st">'rx'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">"Negative"</span> <span class="cf">if</span> <span class="st">'Neg'</span> <span class="kw">not</span> <span class="kw">in</span> plt.gca().get_legend_handles_labels()[<span class="dv">1</span>] <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x2'</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Regression Decision Boundary with Data Points'</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Assignment2_BUDA451_files/figure-html/cell-7-output-1.png" width="519" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The plot above shows the decision boundary <span class="math inline">\(z(x_1, x_2) = 0\)</span> as a blue line. The half-planes are shaded to indicate the regions where the logistic regression model predicts positive (light blue) and negative (light coral) examples. The green circles represent positive examples, while the red crosses represent negative examples.</p>
</section>
</section>
<section id="problem-3b-3-what-is-the-accuracy-of-this-linear-classifier" class="level3">
<h3 class="anchored" data-anchor-id="problem-3b-3-what-is-the-accuracy-of-this-linear-classifier">Problem 3b-3 What is the accuracy of this linear classifier</h3>
<p>The accuracy of a linear classifier is the proportion of correctly classified instances to the total number of instances. In this case, we can calculate the accuracy based on the decision boundary <span class="math inline">\(z(x_1, x_2) = 0\)</span>. We can classify points as positive if <span class="math inline">\(z(x_1, x_2) &gt; 0\)</span> and negative if <span class="math inline">\(z(x_1, x_2) &lt; 0\)</span>.</p>
<p>I’m going to make a hypothetical dataset of 6 points with known labels and see how many are correctly classified by the decision boundary.</p>
<div id="af190006" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose we have a small dataset of 6 points</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Each entry: (x1, x2, true_label)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># true_label = 1 for positive, 0 for negative</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> [</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>),  <span class="co"># point1</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">0</span>),  <span class="co"># point2</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>),  <span class="co"># point3</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>),  <span class="co"># point4</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>),  <span class="co"># point5</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">0</span>)   <span class="co"># point6</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (x1, x2, y_true) <span class="kw">in</span> dataset:</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    z_val <span class="op">=</span> decision_function(x1, x2)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predicted label: 1 if z &gt; 0, else 0</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> z_val <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y_pred <span class="op">==</span> y_true:</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> correct <span class="op">/</span> <span class="bu">len</span>(dataset)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of points:"</span>, <span class="bu">len</span>(dataset))</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Correctly classified:"</span>, correct)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy on this small dataset:"</span>, accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of points: 6
Correctly classified: 4
Accuracy on this small dataset: 0.6666666666666666</code></pre>
</div>
</div>
<p>The accuracy of the linear classifier on this small dataset is 4 out of 6, which is approximately 66.67%. This means that the classifier correctly classified 4 out of the 6 points based on the decision boundary <span class="math inline">\(z(x_1, x_2) = 0\)</span>.</p>
</section>
<section id="problem-3b-4-if-you-apply-the-model-to-a-test-dataset-and-get-the-confusion-matrix-as-follows" class="level3">
<h3 class="anchored" data-anchor-id="problem-3b-4-if-you-apply-the-model-to-a-test-dataset-and-get-the-confusion-matrix-as-follows">Problem 3b-4 If you apply the model to a test dataset and get the confusion matrix as follows:</h3>
<p>::: <img src="images/Problem_3_b4.png" class="img-fluid quarto-figure quarto-figure-center" data-fig-cap="(a) Confusion matrix for the test dataset." alt="confusion matrix dataset"> :::</p>
<section id="problem-3b-4-1-calculate-the-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="problem-3b-4-1-calculate-the-accuracy">Problem 3b-4-1 Calculate the accuracy</h4>
<p>The accuracy of a classifier is defined as the ratio of the number of correct predictions to the total number of predictions. It is a measure of how often the classifier is correct. The accuracy can be calculated using the formula: <span class="math display">\[\frac{45 + 40}{45 + 5 + 10 + 40} = \frac{85}{100} = 85%)\]</span></p>
</section>
<section id="problem-3b-4-2-calculate-the-precision-and-recall" class="level4">
<h4 class="anchored" data-anchor-id="problem-3b-4-2-calculate-the-precision-and-recall">Problem 3b-4-2 Calculate the precision and recall</h4>
<p><strong>Precision</strong> is the ratio of true positive predictions to the total number of positive predictions made by the classifier. It is a measure of how many of the positive predictions are actually correct. The precision can be calculated using the formula: <span class="math display">\[\frac{45}{45 + 10} = \frac{45}{55} = 0.818\]</span></p>
<p><strong>Recall</strong> is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It is a measure of how many of the actual positive instances are correctly predicted by the classifier. The recall can be calculated using the formula: <span class="math display">\[\frac{45}{45 + 5} = \frac{45}{50} = 0.9\]</span></p>
<p>Below is the Python code to calculate the accuracy, precision, and recall based on the given confusion matrix.</p>
<div id="21417c9a" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix counts</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>TP <span class="op">=</span> <span class="dv">45</span>  <span class="co"># True Positives</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>FN <span class="op">=</span> <span class="dv">5</span>   <span class="co"># False Negatives</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>FP <span class="op">=</span> <span class="dv">10</span>  <span class="co"># False Positives</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>TN <span class="op">=</span> <span class="dv">40</span>  <span class="co"># True Negatives</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>accuracy_cm <span class="op">=</span> (TP <span class="op">+</span> TN) <span class="op">/</span> (TP <span class="op">+</span> TN <span class="op">+</span> FP <span class="op">+</span> FN)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>precision_cm <span class="op">=</span> TP <span class="op">/</span> (TP <span class="op">+</span> FP)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>recall_cm <span class="op">=</span> TP <span class="op">/</span> (TP <span class="op">+</span> FN)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix Based Performance:"</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy ="</span>, accuracy_cm)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision ="</span>, precision_cm)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall ="</span>, recall_cm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix Based Performance:
Accuracy = 0.85
Precision = 0.8181818181818182
Recall = 0.9</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="problem-4" class="level2">
<h2 class="anchored" data-anchor-id="problem-4">Problem 4</h2>
<div id="91d7140c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># read training data</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>train_file <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/binbenliu/Teaching/main/data/Diabetes/diabetes_train.csv"</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(train_file, header<span class="op">=</span><span class="st">'infer'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5e9a457d" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read test data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>test_file <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/binbenliu/Teaching/main/data/Diabetes/diabetes_test.csv"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(test_file, header<span class="op">=</span><span class="st">'infer'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="173d6716" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># display the first few rows of the training data</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>train_df.head()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> train_df.columns</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>cols</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
      dtype='object')</code></pre>
</div>
</div>
<div id="4b31d750" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>x_cols <span class="op">=</span> [<span class="st">'Pregnancies'</span>, <span class="st">'Glucose'</span>, <span class="st">'BloodPressure'</span>, <span class="st">'SkinThickness'</span>, <span class="st">'Insulin'</span>,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>       <span class="st">'BMI'</span>, <span class="st">'DiabetesPedigreeFunction'</span>, <span class="st">'Age'</span>]</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>y_col <span class="op">=</span> <span class="st">'Outcome'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a33bbde4" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train data</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[[<span class="st">'Pregnancies'</span>, <span class="st">'Glucose'</span>, <span class="st">'BloodPressure'</span>, <span class="st">'SkinThickness'</span>, <span class="st">'Insulin'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>       <span class="st">'BMI'</span>, <span class="st">'DiabetesPedigreeFunction'</span>, <span class="st">'Age'</span>]].values</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">'Outcome'</span>].values</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># test data</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_df[[<span class="st">'Pregnancies'</span>, <span class="st">'Glucose'</span>, <span class="st">'BloodPressure'</span>, <span class="st">'SkinThickness'</span>, <span class="st">'Insulin'</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>       <span class="st">'BMI'</span>, <span class="st">'DiabetesPedigreeFunction'</span>, <span class="st">'Age'</span>]].values</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_df[<span class="st">'Outcome'</span>].values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-1-decision-tree" class="level2">
<h2 class="anchored" data-anchor-id="model-1-decision-tree">Model 1: Decision Tree</h2>
<div id="b87a20ac" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -U scikit-learn</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co">## here is your code to train the decision tree</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a decision tree classifier</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>dt_classifier <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the classifier on the training data</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>dt_classifier.fit(X_train, y_train)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the labels for the test data</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt_classifier.predict(X_test)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the accuracy of the classifier</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Decision Tree Accuracy:"</span>, accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Decision Tree Accuracy: 0.7142857142857143</code></pre>
</div>
</div>
<div id="aedb9d32" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># here is your code to make prediciton and get the classfication metrics based on the trained decision tree model</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the labels for the test data</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> dt_classifier.predict(X_test)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the classification metrics</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_test, y_pred)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_test, y_pred)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_test, y_pred)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Decision Tree Classification Metrics:"</span>)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, accuracy)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, precision)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, recall)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1 Score:"</span>, f1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Decision Tree Classification Metrics:
Accuracy: 0.7142857142857143
Precision: 0.5964912280701754
Recall: 0.6181818181818182
F1 Score: 0.6071428571428571</code></pre>
</div>
</div>
<p><strong>Displaying the decision tree</strong></p>
<div id="029c2dfe" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a shallower tree by limiting max_depth</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>dt_classifier <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>dt_classifier.fit(X_train, y_train)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plot_tree(</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    dt_classifier,</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    feature_names<span class="op">=</span>x_cols,</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    class_names<span class="op">=</span>[<span class="st">"0"</span>, <span class="st">"1"</span>],  <span class="co"># or ["No Diabetes", "Diabetes"]</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    rounded<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    proportion<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    precision<span class="op">=</span><span class="dv">2</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree (max_depth=3)"</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Assignment2_BUDA451_files/figure-html/cell-17-output-1.png" width="912" height="631" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the code above, I trained a decision tree classifier with a maximum depth of 3 and visualized the tree using the <code>plot_tree</code> function from scikit-learn. The resulting decision tree is displayed above. The issue with limiting the depth of the tree is that it may not capture all the underlying patterns in the data, leading to lower accuracy and performance.</p>
<p><strong>Pruning the Decision Tree</strong></p>
<div id="d70ab41a" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>dt_classifier <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>dt_classifier.fit(X_train, y_train)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> dt_classifier.cost_complexity_pruning_path(X_train, y_train)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>ccp_alphas, impurities <span class="op">=</span> path.ccp_alphas, path.impurities</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>clfs <span class="op">=</span> []</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ccp_alpha <span class="kw">in</span> ccp_alphas:</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>ccp_alpha)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    clf.fit(X_train, y_train)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    clfs.append(clf)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> plot_tree</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> <span class="fl">0.0030</span>  <span class="co"># smaller alpha -&gt; more splits</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>pruned_tree <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>best_alpha)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>pruned_tree.fit(X_train, y_train)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>plot_tree(pruned_tree, filled<span class="op">=</span><span class="va">True</span>, feature_names<span class="op">=</span>x_cols, class_names<span class="op">=</span>[<span class="st">"0"</span>,<span class="st">"1"</span>])</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Pruned Decision Tree (ccp_alpha=</span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Assignment2_BUDA451_files/figure-html/cell-18-output-1.png" width="912" height="631" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-2-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="model-2-logistic-regression">Model 2: Logistic Regression</h2>
<p>We consider a logistic regression model with <strong>L2 regularization</strong> by default. The decision function is:</p>
<p><span class="math display">\[
L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n l\left(\sigma\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b\right),  y^{(i)}\right) + \frac{\alpha}{2} \|\mathbf{w}\|^2.
\]</span></p>
<p>where (b) is the intercept, (w_i) are the weights, and (x_i) are the feature values.<br>
The predicted probability for the positive class ((y=1)) is given by:</p>
<p><span class="math display">\[
p(y=1 \mid x) = \sigma(z) = \frac{1}{1 + e^{-z}}.
\]</span></p>
<div id="6bda0256" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create and train the Logistic Regression model</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>lr_model <span class="op">=</span> LogisticRegression(</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    max_iter<span class="op">=</span><span class="dv">1000</span>, </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>lr_model.fit(X_train, y_train)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Predict on the test set</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>y_pred_lr <span class="op">=</span> lr_model.predict(X_test)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Evaluate performance</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>accuracy_lr  <span class="op">=</span> accuracy_score(y_test, y_pred_lr)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>precision_lr <span class="op">=</span> precision_score(y_test, y_pred_lr)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>recall_lr    <span class="op">=</span> recall_score(y_test, y_pred_lr)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>f1_lr        <span class="op">=</span> f1_score(y_test, y_pred_lr)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression Metrics on Test Data"</span>)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"----------------------------------------"</span>)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy :  </span><span class="sc">{</span>accuracy_lr<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision:  </span><span class="sc">{</span>precision_lr<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall   :  </span><span class="sc">{</span>recall_lr<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 Score :  </span><span class="sc">{</span>f1_lr<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Feature importances (coefficients)</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>coef_values <span class="op">=</span> lr_model.coef_[<span class="dv">0</span>]</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> lr_model.intercept_[<span class="dv">0</span>]</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Logistic Regression Feature Importances:"</span>)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature_name, coef_val <span class="kw">in</span> <span class="bu">zip</span>(x_cols, coef_values):</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>feature_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef_val<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Intercept (b): </span><span class="sc">{</span>intercept<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Logistic Regression Metrics on Test Data
----------------------------------------
Accuracy :  0.7727
Precision:  0.7500
Recall   :  0.5455
F1 Score :  0.6316

Logistic Regression Feature Importances:
Pregnancies: 0.1206
Glucose: 0.0329
BloodPressure: -0.0106
SkinThickness: -0.0024
Insulin: -0.0008
BMI: 0.1024
DiabetesPedigreeFunction: 0.9574
Age: 0.0098

Intercept (b): -8.5250</code></pre>
</div>
</div>
<p>Here I utilized the <code>LogisticRegression</code> class from scikit-learn to train a logistic regression model on the training data. I then used this model to predict the labels for the test data and calculated various classification metrics such as accuracy, precision, recall, and F1 score. Additionally, I extracted the feature importances (coefficients) and the intercept term from the trained logistic regression model. Now the max iterations is set to 1000 to ensure convergence. However if I use too many iterations, it may lead to <strong>overfitting</strong>.</p>
<section id="lr-predict-and-evaulate-the-following-classification-metrics-accuracy-precision-recall-and-f1-score" class="level3">
<h3 class="anchored" data-anchor-id="lr-predict-and-evaulate-the-following-classification-metrics-accuracy-precision-recall-and-f1-score">LR: Predict and evaulate the following classification metrics: accuracy, precision, recall, and F1 score</h3>
<div id="b17759b2" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># here is your code to make prediciton and get the classfication metrics based on the trained logistic regression model</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the labels for the test data</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>y_pred_lr <span class="op">=</span> lr_model.predict(X_test)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the classification metrics</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>accuracy_lr <span class="op">=</span> accuracy_score(y_test, y_pred_lr)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>precision_lr <span class="op">=</span> precision_score(y_test, y_pred_lr)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>recall_lr <span class="op">=</span> recall_score(y_test, y_pred_lr)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>f1_lr <span class="op">=</span> f1_score(y_test, y_pred_lr)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression Classification Metrics:"</span>)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, accuracy_lr)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, precision_lr)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, recall_lr)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1 Score:"</span>, f1_lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Logistic Regression Classification Metrics:
Accuracy: 0.7727272727272727
Precision: 0.75
Recall: 0.5454545454545454
F1 Score: 0.631578947368421</code></pre>
</div>
</div>
</section>
<section id="feature-importance-display-the-feature-importance-scores-namely-w_1-w_2-...-w_d-and-discuss-what-you-find-from-the-model." class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-display-the-feature-importance-scores-namely-w_1-w_2-...-w_d-and-discuss-what-you-find-from-the-model.">Feature importance: Display the feature importance scores, namely <span class="math inline">\(w_1, w_2, ..., w_d\)</span>, and discuss what you find from the model.</h3>
<p>The feature importance scores represent the weights assigned to each feature by the logistic regression model. These weights indicate the impact of each feature on the predicted probability of the positive class. Features with higher absolute weights have a stronger influence on the model’s predictions.</p>
<div id="38581442" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the feature importances (coefficients) and the intercept term</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>coef_values <span class="op">=</span> lr_model.coef_[<span class="dv">0</span>]</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> lr_model.intercept_[<span class="dv">0</span>]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Logistic Regression Feature Importances:"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature_name, coef_val <span class="kw">in</span> <span class="bu">zip</span>(x_cols, coef_values):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>feature_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef_val<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Intercept (b): </span><span class="sc">{</span>intercept<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Logistic Regression Feature Importances:
Pregnancies: 0.1206
Glucose: 0.0329
BloodPressure: -0.0106
SkinThickness: -0.0024
Insulin: -0.0008
BMI: 0.1024
DiabetesPedigreeFunction: 0.9574
Age: 0.0098

Intercept (b): -8.5250</code></pre>
</div>
</div>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Pruning is a technique used in decision trees to remove branches that have little importance and do not provide significant power to the model. This helps in reducing the complexity of the tree and prevents it from fitting noise in the training data.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Setting a maximum depth in decision trees restricts the number of levels in the tree. This prevents the model from becoming too complex and helps it generalize better to unseen data.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>